# -*- coding: utf-8 -*-
"""Traffic Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ynlu_U8_T7pxWdxlxOWsYGSMXsqnLgHv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import math
import os

# Set random seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# Configuration
FILE_PATH = 'PEMS-BAY (1).csv'
SEQ_LENGTH = 12
BATCH_SIZE = 64
EPOCHS = 30
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {DEVICE}")

def load_and_enrich_data(filepath):
    """
    Loads data, cleans noise, and adds rich temporal embeddings.
    """
    if not os.path.exists(filepath):
        print(f"Error: File {filepath} not found.")
        return None, None

    print(f"Loading data from {filepath}...")
    try:
        # Load CSV. Assumes first column is timestamp.
        df = pd.read_csv(filepath, index_col=0, parse_dates=True)
        df.sort_index(inplace=True)
        df = df.astype('float32')
    except Exception as e:
        print(f"Error loading file: {e}")
        return None, None

    sensor_cols = df.columns.tolist()
    print(f"Loaded dataset with {len(sensor_cols)} sensors and {len(df)} time steps.")

    # --- DATA CLEANING ---
    print("Cleaning data: Replacing 0.0 with NaN and interpolating...")
    df[sensor_cols] = df[sensor_cols].replace(0.0, np.nan)
    df[sensor_cols] = df[sensor_cols].interpolate(method='linear', limit_direction='both')
    df[sensor_cols] = df[sensor_cols].ffill().bfill()

    # --- ADVANCED FEATURE ENGINEERING ---
    print("Feature Engineering: Adding Day-of-Week and Hour-of-Day embeddings...")

    # Calculate features
    hour = df.index.hour + df.index.minute / 60.0
    day_of_week = df.index.dayofweek

    # Create a separate DataFrame for new features to avoid fragmentation warning
    time_features = pd.DataFrame(index=df.index)
    time_features['sin_hour'] = np.sin(2 * np.pi * hour / 24.0)
    time_features['cos_hour'] = np.cos(2 * np.pi * hour / 24.0)
    time_features['sin_day'] = np.sin(2 * np.pi * day_of_week / 7.0)
    time_features['cos_day'] = np.cos(2 * np.pi * day_of_week / 7.0)

    # Concatenate at once
    df = pd.concat([df, time_features], axis=1)

    print(f"Final data shape: {df.shape}")
    return df, sensor_cols

def create_sequences(input_data, target_data, seq_length):
    """
    Creates sliding window sequences efficiently.
    """
    xs, ys = [], []
    total_len = len(input_data)

    for i in range(total_len - seq_length):
        x = input_data[i:(i + seq_length)]
        y = target_data[i + seq_length]
        xs.append(x)
        ys.append(y)

    return np.array(xs), np.array(ys)

class HybridTrafficModel(nn.Module):
    def __init__(self, num_input_features, num_output_sensors, seq_length):
        super(HybridTrafficModel, self).__init__()

        # --- Local Trend Extraction (CNN) ---
        # PyTorch Conv1d expects input: (Batch, Channels, Length)
        # We will permute inputs in forward() to match this.
        self.conv1 = nn.Conv1d(in_channels=num_input_features, out_channels=64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.relu = nn.ReLU()

        # --- Temporal Dynamics (GRU) ---
        # Layer 1: Returns sequences (equivalent to return_sequences=True)
        self.gru1 = nn.GRU(input_size=64, hidden_size=128, batch_first=True)
        self.dropout1 = nn.Dropout(0.2)

        # Layer 2: Returns last state (equivalent to return_sequences=False)
        self.gru2 = nn.GRU(input_size=128, hidden_size=64, batch_first=True)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(0.2)

        # --- Prediction Head ---
        self.fc1 = nn.Linear(64, 64)
        self.fc_out = nn.Linear(64, num_output_sensors)

    def forward(self, x):
        # Input x shape: (Batch, Seq_Length, Features)

        # 1. CNN Layer
        # Permute to (Batch, Features, Seq_Length) for Conv1d
        x = x.permute(0, 2, 1)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        # Permute back to (Batch, Seq_Length, Features) for GRU
        x = x.permute(0, 2, 1)

        # 2. GRU Layers
        x, _ = self.gru1(x) # Output: (Batch, Seq_Length, 128)
        x = self.dropout1(x)

        x, _ = self.gru2(x) # Output: (Batch, Seq_Length, 64)

        # Take the last time step (Many-to-One)
        x = x[:, -1, :]

        x = self.bn2(x)
        x = self.dropout2(x)

        # 3. Dense Layers
        x = self.relu(self.fc1(x))
        x = self.fc_out(x)

        return x

class EarlyStopping:
    """Early stopping to stop the training when the loss does not improve after patience epochs."""
    def __init__(self, patience=5, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model_state = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_model_state = model.state_dict()
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.best_model_state = model.state_dict()
            self.counter = 0

def run_traffic_prediction():
    # 1. Load Data
    df, sensor_cols = load_and_enrich_data(FILE_PATH)
    if df is None:
        return

    # 2. Data Splitting
    n = len(df)
    train_size = int(n * 0.8)
    val_size = int(n * 0.1)

    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:train_size+val_size]
    test_df = df.iloc[train_size+val_size:]

    print(f"Split: {len(train_df)} Train, {len(val_df)} Val, {len(test_df)} Test")

    # 3. Scaling
    print("Scaling sensor data...")
    scaler = MinMaxScaler()
    scaler.fit(train_df[sensor_cols])

    def prepare_dataset(data_df):
        sensors_scaled = scaler.transform(data_df[sensor_cols])
        # Temporal features
        time_cols = ['sin_hour', 'cos_hour', 'sin_day', 'cos_day']
        time_features = data_df[time_cols].values
        # Concatenate: [Sensors, Time]
        inputs = np.hstack([sensors_scaled, time_features])
        targets = sensors_scaled
        return inputs, targets

    train_input, train_target = prepare_dataset(train_df)
    val_input, val_target = prepare_dataset(val_df)
    test_input, test_target = prepare_dataset(test_df)

    # 4. Create Sequences & DataLoaders
    print(f"Creating sequences with length {SEQ_LENGTH}...")

    X_train, y_train = create_sequences(train_input, train_target, SEQ_LENGTH)
    X_val, y_val = create_sequences(val_input, val_target, SEQ_LENGTH)
    X_test, y_test = create_sequences(test_input, test_target, SEQ_LENGTH)

    # Convert to PyTorch Tensors
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 5. Build Model
    num_input_features = X_train.shape[2]
    num_output_sensors = y_train.shape[1]

    print("Building Hybrid CNN-GRU Model (PyTorch)...")
    model = HybridTrafficModel(num_input_features, num_output_sensors, SEQ_LENGTH).to(DEVICE)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # FIX: Remove 'verbose' argument which caused the TypeError in recent PyTorch versions
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
    early_stopper = EarlyStopping(patience=5)

    # 6. Training Loop
    print("\nStarting training...")
    history = {'loss': [], 'val_loss': []}

    for epoch in range(EPOCHS):
        # Train
        model.train()
        train_losses = []
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        avg_train_loss = np.mean(train_losses)

        # Validation
        model.eval()
        val_losses = []
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_losses.append(loss.item())

        avg_val_loss = np.mean(val_losses)

        history['loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)

        # Print current LR
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}")

        # Scheduler & Early Stopping
        scheduler.step(avg_val_loss)
        early_stopper(avg_val_loss, model)

        if early_stopper.early_stop:
            print("Early stopping triggered")
            model.load_state_dict(early_stopper.best_model_state)
            break

    # Save Model
    torch.save(model.state_dict(), 'traffic_model.pth')
    print("Model saved to traffic_model.pth")

    # 7. Predict & Evaluate
    print("\nEvaluating on Test Set...")
    model.eval()
    predictions = []

    with torch.no_grad():
        for batch_x, _ in test_loader:
            batch_x = batch_x.to(DEVICE)
            outputs = model(batch_x)
            predictions.append(outputs.cpu().numpy())

    y_pred_scaled = np.vstack(predictions)

    # Ensure shapes match (handle potential batch dropping)
    if len(y_pred_scaled) != len(y_test):
        print("Warning: Prediction length mismatch. Adjusting to match test set.")
        y_test_adjusted = y_test[:len(y_pred_scaled)]
    else:
        y_test_adjusted = y_test

    # Inverse transform
    y_true = scaler.inverse_transform(y_test_adjusted)
    y_pred = scaler.inverse_transform(y_pred_scaled)

    # 8. Analytics
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = math.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print("\n" + "="*40)
    print("FINAL PERFORMANCE METRICS (Test Set)")
    print("="*40)
    metrics_df = pd.DataFrame({
        "Metric": ["MAE", "RMSE", "R2 Score"],
        "Value": [mae, rmse, r2]
    })
    print(metrics_df.to_string(index=False))
    print("="*40)

    # 9. Visualizations
    plt.figure(figsize=(16, 10))

    # Loss Curve
    plt.subplot(2, 2, 1)
    plt.plot(history['loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Learning Curve (PyTorch)')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid(True)

    # Mean Network Speed Comparison
    mean_true = np.mean(y_true, axis=1)
    mean_pred = np.mean(y_pred, axis=1)

    plt.subplot(2, 2, 2)
    plt.plot(mean_true[:500], label='Actual (Mean Speed)', alpha=0.7)
    plt.plot(mean_pred[:500], label='Predicted (Mean Speed)', alpha=0.7, linestyle='--')
    plt.title('Network-Wide Average Traffic Speed (First 500 Steps)')
    plt.legend()
    plt.grid(True)

    # Specific Sensor Zoom
    sensor_idx = 0
    plt.subplot(2, 2, 3)
    plt.plot(y_true[:300, sensor_idx], label='Actual', color='blue')
    plt.plot(y_pred[:300, sensor_idx], label='Predicted', color='orange', linestyle='--')
    plt.title(f'Single Sensor Prediction (ID: {sensor_cols[sensor_idx]})')
    plt.legend()
    plt.grid(True)

    # Correlation Scatter Plot
    plt.subplot(2, 2, 4)
    flat_true = y_true.flatten()
    flat_pred = y_pred.flatten()

    if len(flat_true) > 10000:
        indices = np.random.choice(len(flat_true), 10000, replace=False)
        sample_true = flat_true[indices]
        sample_pred = flat_pred[indices]
    else:
        sample_true = flat_true
        sample_pred = flat_pred

    plt.scatter(sample_true, sample_pred, alpha=0.1, s=1, color='purple')

    min_val = min(sample_true.min(), sample_pred.min())
    max_val = max(sample_true.max(), sample_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)

    plt.title(f'Prediction Accuracy (R2: {r2:.4f})')
    plt.xlabel('Actual Speed')
    plt.ylabel('Predicted Speed')
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('traffic_analysis.png')
    print("\nAnalysis saved as 'traffic_analysis.png'")
    plt.show()

if __name__ == "__main__":
    run_traffic_prediction()